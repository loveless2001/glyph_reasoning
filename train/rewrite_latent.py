import json
import random
import torch
from tqdm import tqdm
from transformers import AutoTokenizer, AutoModelForCausalLM
from prompts import latent_prompt, natural_prompt

# Input is the FILTERED glyph traces (which contain explicit glyphs)
INPUT_FILE = "data/glyph_traces_filtered.jsonl"
# Output is the training file for SFT
OUTPUT_FILE = "data/sft_final.jsonl"

def main():
    print(f"Reading from {INPUT_FILE}...")
    with open(INPUT_FILE, "r") as f:
        # Read all lines
        lines = f.readlines()
    
    data = [json.loads(line) for line in lines]
    
    # Shuffle for randomness
    random.seed(42)
    random.shuffle(data)
    
    print(f"Processing {len(data)} records...")
    
    with open(OUTPUT_FILE, "w") as out:
        for r in tqdm(data):
            # We assume the input file has correctly formatted messages from 'generate_traces.py'
            # Format: {"messages": [{"role": "user", "content": "..."}, {"role": "assistant", "content": "ðŸœž..."}]}
            
            if "messages" not in r:
                continue
            
            # 1. Get the High-Quality Glyph Response (Generated by Teacher)
            # This response contains ðŸœž, ðŸœ†, etc.
            glyph_response = r["messages"][1]["content"]
            
            # 2. Generate the "Latent Prompt"
            # This prompt is "Solve carefully. Do not mention tags."
            # We pair this prompt with the GLYPH response.
            # This teaches the model: "When asked to solve carefully (neutral), use Glyphs."
            lp = latent_prompt(r["question"])
            
            # 3. Write the SFT Entry
            entry = {
                "messages": [
                    {"role": "user", "content": lp},
                    {"role": "assistant", "content": glyph_response}
                ]
            }
            out.write(json.dumps(entry) + "\n")
            
    print(f"Done. Written to {OUTPUT_FILE}")

if __name__ == "__main__":
    main()
