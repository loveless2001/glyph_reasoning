{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Interactive Model Testing\n",
                "\n",
                "Use this notebook to test your trained `Qwen2.5-0.5B-Glyph` model. \n",
                "We will load the model (and LoRA adapter if applicable) and generate responses to see if the **Glyphs** emerge."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import torch\n",
                "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
                "from peft import PeftModel\n",
                "\n",
                "# CONFIG\n",
                "BASE_MODEL = \"Qwen/Qwen2.5-0.5B-Instruct\"\n",
                "CHECKPOINT_PATH = \"checkpoints/qwen2.5-0.5b-glyph-sft\"\n",
                "USE_LORA = False # Set to True if you trained with LoRA, False if Full SFT\n",
                "\n",
                "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
                "print(f\"Using device: {device}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Load Tokenizer & Model\n",
                "tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL, trust_remote_code=True)\n",
                "\n",
                "print(\"Loading Base Model...\")\n",
                "model = AutoModelForCausalLM.from_pretrained(\n",
                "    BASE_MODEL,\n",
                "    torch_dtype=torch.float16,\n",
                "    device_map=\"auto\",\n",
                "    trust_remote_code=True\n",
                ")\n",
                "\n",
                "if USE_LORA:\n",
                "    print(f\"Loading LoRA Adapter from {CHECKPOINT_PATH}...\")\n",
                "    model = PeftModel.from_pretrained(model, CHECKPOINT_PATH)\n",
                "else:\n",
                "    print(f\"Loading Full Finetuned Weights (if merged) or attempting to load from {CHECKPOINT_PATH}...\")\n",
                "    # If Checkpoint is a full model save:\n",
                "    try:\n",
                "        model = AutoModelForCausalLM.from_pretrained(\n",
                "            CHECKPOINT_PATH,\n",
                "            torch_dtype=torch.float16,\n",
                "            device_map=\"auto\",\n",
                "            trust_remote_code=True\n",
                "        )\n",
                "        print(\"Loaded finetuned model successfully.\")\n",
                "    except Exception as e:\n",
                "        print(f\"Could not load full model from checkpoint (maybe it is LoRA?): {e}\")\n",
                "        print(\"Falling back to Base Model (Result will be Untrained behavior) or check path.\")\n",
                "\n",
                "model.eval()\n",
                "print(\"Model Ready!\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def generate(prompt, max_new_tokens=512):\n",
                "    messages = [\n",
                "        {\"role\": \"user\", \"content\": prompt}\n",
                "    ]\n",
                "    text = tokenizer.apply_chat_template(\n",
                "        messages,\n",
                "        tokenize=False,\n",
                "        add_generation_prompt=True\n",
                "    )\n",
                "    model_inputs = tokenizer([text], return_tensors=\"pt\").to(device)\n",
                "\n",
                "    with torch.no_grad():\n",
                "        generated_ids = model.generate(\n",
                "            **model_inputs,\n",
                "            max_new_tokens=max_new_tokens,\n",
                "            do_sample=False, # Deterministic for evaluation\n",
                "            temperature=None,\n",
                "            top_p=None,\n",
                "            pad_token_id=tokenizer.eos_token_id\n",
                "        )\n",
                "\n",
                "    generated_ids = [output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)]\n",
                "    response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
                "    return response"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Test Cases\n",
                "Try a **Latent Prompt** (Neutral) on a **Hard** problem. We expect to see glyphs (`ðŸœž`, `ðŸœ†`, etc.) emerge autonomously."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "latent_prompt = \"\"\"\n",
                "Solve the following problem carefully.\n",
                "Do not mention any tags, symbols, or special formatting.\n",
                "\n",
                "Problem:\n",
                "Natalia sold clips to 48 of her friends in April, and then she sold half as many clips in May. How many clips did Natalia sell altogether in April and May?\n",
                "\"\"\"\n",
                "\n",
                "response = generate(latent_prompt)\n",
                "print(\"Response:\\n\" + \"-\"*20)\n",
                "print(response)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Try your own prompt here\n",
                "my_prompt = \"\"\"\n",
                "Solve the following problem carefully.\n",
                "Do not mention any tags, symbols, or special formatting.\n",
                "\n",
                "Problem:\n",
                "There are 5 houses on a street. Each house has 3 windows. If 4 windows are broken in total, how many unbroken windows are there?\n",
                "\"\"\"\n",
                "\n",
                "print(generate(my_prompt))"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.11.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}